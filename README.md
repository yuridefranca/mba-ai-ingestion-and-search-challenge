# RAG System - PDF Ingestion and Semantic Search

Sistema de Recupera√ß√£o Aumentada por Gera√ß√£o (RAG) para busca sem√¢ntica em documentos PDF usando LangChain e PostgreSQL com pgVector.

## üìã Vis√£o Geral

Este projeto implementa um sistema RAG completo que:
- Ingere documentos PDF e armazena em banco vetorial
- Permite fazer perguntas via CLI sobre o conte√∫do do PDF
- Responde apenas com base no contexto do documento (sem alucina√ß√µes)

## üõ†Ô∏è Tecnologias

- **Python 3.12+**
- **LangChain** - Framework de orquestra√ß√£o
- **PostgreSQL + pgVector** - Banco de dados vetorial
- **Docker & Docker Compose** - Containeriza√ß√£o
- **Modelos de Embedding**: OpenAI, Google Gemini, ou HuggingFace (local)
- **LLM**: Google Gemini

## üìÅ Estrutura do Projeto

```
‚îú‚îÄ‚îÄ docker-compose.yml          # Configura√ß√£o PostgreSQL + pgVector
‚îú‚îÄ‚îÄ requirements.txt            # Depend√™ncias Python
‚îú‚îÄ‚îÄ .env.example               # Template de vari√°veis de ambiente
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ ingest.py              # Script de ingest√£o do PDF
‚îÇ   ‚îú‚îÄ‚îÄ search.py              # L√≥gica de busca sem√¢ntica
‚îÇ   ‚îú‚îÄ‚îÄ chat.py                # Interface CLI
‚îÇ   ‚îî‚îÄ‚îÄ utils.py               # Utilit√°rios (sele√ß√£o de modelo)
‚îú‚îÄ‚îÄ document.pdf               # PDF para ingest√£o
‚îî‚îÄ‚îÄ README.md                  # Este arquivo
```

---

## üöÄ Setup e Instala√ß√£o

### 1. Pr√©-requisitos

- Python 3.12 ou superior
- Docker e Docker Compose instalados
- Git

### 2. Clone o Reposit√≥rio

```bash
git clone https://github.com/devfullcycle/mba-ia-desafio-ingestao-busca.git
cd mba-ia-desafio-ingestao-busca
```

### 3. Crie o Ambiente Virtual

```bash
python3 -m venv .venv
source .venv/bin/activate  # No Windows: .venv\Scripts\activate
```

### 4. Instale as Depend√™ncias

```bash
pip install -r requirements.txt
```

---

## ‚öôÔ∏è Configura√ß√£o do Ambiente

### 1. Copie o Arquivo de Exemplo

```bash
cp .env.example .env
```

### 2. Configure as Vari√°veis de Ambiente

Edite o arquivo `.env` com suas configura√ß√µes:

```bash
# Tipo de modelo de embedding (free, gemini, ou openai)
MODEL_TYPE=free

# Google Gemini (opcional - apenas se MODEL_TYPE=gemini)
GOOGLE_API_KEY=sua_api_key_aqui
GOOGLE_EMBEDDING_MODEL='models/embedding-001'
GOOGLE_GEMINI_MODEL='gemini-2.5-flash-lite'

# OpenAI (opcional - apenas se MODEL_TYPE=openai)
OPENAI_API_KEY=sua_api_key_aqui
OPENAI_EMBEDDING_MODEL='text-embedding-3-small'

# Modelo local gratuito (padr√£o quando MODEL_TYPE=free)
FREE_EMBEDDING_MODEL='google/embeddinggemma-300m'

# Configura√ß√£o do Banco de Dados
DATABASE_URL='postgresql://postgres:postgres@localhost:5432/rag'
PG_VECTOR_COLLECTION_NAME='pdf_documents'

# Caminho do PDF
PDF_PATH='document.pdf'
```

### 3. Op√ß√µes de Modelos de Embedding

Voc√™ pode escolher entre tr√™s op√ß√µes configurando `MODEL_TYPE`:

| Op√ß√£o | Configura√ß√£o | Vantagens | Desvantagens |
|-------|-------------|-----------|--------------|
| **free** | `MODEL_TYPE=free` | Gratuito, sem API key, roda localmente | Menor qualidade |
| **gemini** | `MODEL_TYPE=gemini` | Alta qualidade, quota gratuita | Requer API key, limites de quota |
| **openai** | `MODEL_TYPE=openai` | Melhor qualidade | Pago, requer API key |

**Recomenda√ß√£o**: Use `free` para desenvolvimento e testes, `gemini` ou `openai` para produ√ß√£o.

---

## üóÑÔ∏è Inicializa√ß√£o do Banco de Dados

### 1. Suba o PostgreSQL com pgVector

```bash
docker compose up -d
```

Isso ir√°:
- Criar um container PostgreSQL com a extens√£o pgVector
- Expor na porta 5432
- Criar o banco de dados `rag`
- Instalar a extens√£o `vector` automaticamente

### 2. Verifique se o Banco Est√° Rodando

```bash
docker ps
```

Voc√™ deve ver um container chamado `postgres_rag` rodando.

### 3. (Opcional) Acesse o Banco de Dados

```bash
docker exec -it postgres_rag psql -U postgres -d rag
```

Comandos √∫teis dentro do psql:
```sql
-- Listar cole√ß√µes
SELECT * FROM langchain_pg_collection;

-- Verificar extens√µes instaladas
\dx

-- Sair
\q
```

---

## üì• Ingest√£o do PDF

### 1. Coloque seu PDF no Diret√≥rio

Certifique-se de que o arquivo `document.pdf` existe no diret√≥rio raiz, ou atualize `PDF_PATH` no `.env`.

### 2. Execute o Script de Ingest√£o

```bash
python src/ingest.py
```

**O que acontece:**
1. Carrega o PDF usando `PyPDFLoader`
2. Divide em chunks de 1000 caracteres (overlap de 150)
3. Gera embeddings para cada chunk
4. Armazena os vetores no PostgreSQL

**Tempo estimado**: 1-5 minutos dependendo do tamanho do PDF e do modelo escolhido.

**Sa√≠da esperada:**
```
[Sem erros = sucesso]
```

### 3. Verifica√ß√£o

```bash
docker exec -it postgres_rag psql -U postgres -d rag -c "SELECT COUNT(*) FROM langchain_pg_embedding;"
```

Voc√™ deve ver o n√∫mero de chunks ingeridos.

---

## üí¨ Execu√ß√£o do Chat

### 1. Inicie o Chat Interativo

```bash
python src/chat.py
```

### 2. Fa√ßa Perguntas

```
==================================================
Chat com Documentos PDF - Sistema RAG
==================================================
Digite suas perguntas sobre o documento.
Digite 'sair', 'exit' ou 'quit' para encerrar.

Voc√™: Qual √© o tema principal do documento?

Buscando informa√ß√µes...

Assistente: [Resposta baseada no conte√∫do do PDF]

--------------------------------------------------

Voc√™: sair
Encerrando o assistente. At√© logo!
```

### 3. Comportamento Esperado

**Perguntas no contexto** (informa√ß√µes presentes no PDF):
```
Voc√™: Qual o faturamento da empresa?
Assistente: O faturamento foi de 10 milh√µes de reais.
```

**Perguntas fora do contexto** (informa√ß√µes n√£o presentes no PDF):
```
Voc√™: Qual √© a capital da Fran√ßa?
Assistente: N√£o tenho informa√ß√µes necess√°rias para responder sua pergunta.
```

---

## üîÑ Fluxo de Trabalho Completo

```bash
# 1. Ative o ambiente virtual
source .venv/bin/activate

# 2. Suba o banco de dados
docker compose up -d

# 3. Configure o .env (se ainda n√£o fez)
cp .env.example .env
# Edite o .env com suas configura√ß√µes

# 4. Ingira o PDF
python src/ingest.py

# 5. Inicie o chat
python src/chat.py
```

---

## üêõ Troubleshooting

### Erro: "No module named 'dotenv'"
**Causa**: N√£o est√° usando o ambiente virtual ou depend√™ncias n√£o instaladas.

**Solu√ß√£o**:
```bash
source .venv/bin/activate
pip install -r requirements.txt
```

### Erro: "429 You exceeded your current quota"
**Causa**: Quota do Google Gemini/OpenAI esgotada.

**Solu√ß√£o**: Mude para modelo local:
```bash
# No .env
MODEL_TYPE=free
```

Depois re-ingira o PDF:
```bash
# Limpe a cole√ß√£o primeiro
docker exec -it postgres_rag psql -U postgres -d rag -c "DROP TABLE IF EXISTS langchain_pg_embedding CASCADE;"

# Re-ingira
python src/ingest.py
```

### Erro: "Vector dimension mismatch"
**Causa**: Mudou o modelo de embedding sem re-ingerir.

**Solu√ß√£o**: Sempre que mudar `MODEL_TYPE`, re-ingira o PDF:
```bash
# Limpe o banco
docker compose down -v
docker compose up -d

# Aguarde o banco inicializar
sleep 10

# Re-ingira
python src/ingest.py
```

### Erro: "Connection refused" no PostgreSQL
**Causa**: Banco n√£o est√° rodando ou n√£o terminou de inicializar.

**Solu√ß√£o**:
```bash
# Verifique se est√° rodando
docker ps

# Se n√£o estiver, inicie
docker compose up -d

# Aguarde a inicializa√ß√£o (healthcheck)
docker compose ps
```

### Banco n√£o conecta mesmo rodando
**Causa**: URL do banco incorreta no `.env`.

**Solu√ß√£o**: Verifique se `DATABASE_URL` est√° correto:
```bash
DATABASE_URL='postgresql://postgres:postgres@localhost:5432/rag'
```

---

## üß™ Testando o Sistema

### Teste 1: Pergunta no Contexto
```
Voc√™: [Pergunte algo que EST√Å no PDF]
Esperado: Resposta precisa baseada no documento
```

### Teste 2: Pergunta Fora do Contexto
```
Voc√™: Qual √© a capital da Fran√ßa?
Esperado: "N√£o tenho informa√ß√µes necess√°rias para responder sua pergunta."
```

### Teste 3: Pergunta Vaga
```
Voc√™: Me fale sobre isso
Esperado: Resposta baseada no contexto geral do documento
```

---

## üìù Notas Importantes

1. **Troca de Modelo**: Sempre re-ingira o PDF ao mudar `MODEL_TYPE`
2. **Performance**: Modelos locais (`free`) s√£o mais lentos mas gratuitos
3. **Seguran√ßa**: Nunca commite o arquivo `.env` com suas API keys
4. **Limites**: Google Gemini free tier tem limite de requisi√ß√µes por minuto
5. **Custos**: OpenAI cobra por uso de embeddings e LLM

---

## ü§ù Contribuindo

Este √© um projeto acad√™mico do MBA Full Cycle. Para contribuir:

1. Fork o reposit√≥rio
2. Crie uma branch para sua feature (`git checkout -b feature/nova-feature`)
3. Commit suas mudan√ßas seguindo [Conventional Commits](.github/git-instructions.md)
4. Push para a branch (`git push origin feature/nova-feature`)
5. Abra um Pull Request

---

## üìÑ Licen√ßa

Este projeto √© parte do MBA Engenharia de Software com IA - Full Cycle.

---

## üìß Suporte

Para d√∫vidas ou problemas:
- Consulte a documenta√ß√£o do [LangChain](https://python.langchain.com/)
- Veja os [requirements](Requirements.md) originais do projeto
- Abra uma issue no reposit√≥rio